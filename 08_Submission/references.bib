@article{chen2021evaluating,
    title={Evaluating Large Language Models Trained on Code},
    author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Kaplan, Jared and de Oliveira Pinto, H. and Zaremba, Wojciech},
    journal={arXiv preprint arXiv:2107.03374},
    year={2021}
}

@article{austin2021program,
    title={Program Synthesis with Large Language Models},
    author={Austin, Jack and Odena, Augustin and Nye, Maxwell and Bosma, Maarten and Chen, Mark and Sutton, Charles and Alemi, Alexander A. and Johnson, Josh and Bieber, Gabe and Zhou, David and others},
    journal={arXiv preprint arXiv:2108.07732},
    year={2021}
}

@article{madaan2023self,
    title={Self-Refine: Iterative Refinement with Self-Feedback},
    author={Madaan, Aman and Tandon, Niket and Zhang, Jinlyu and Wang,GroupBy and Ji, Yiming and Li, Hanfang and Yin, Xiang and Bansal, Mohit and Gupta, Prateek and others},
    journal={arXiv preprint arXiv:2303.17651},
    year={2023}
}

@inproceedings{wei2022chain,
    title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
    author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Le, Quoc V and others},
    booktitle={Advances in Neural Information Processing Systems},
    volume={35},
    pages={24824--24837},
    year={2022}
}

@article{berglund2023reversal,
    title={The Reversal Curse: LLMs trained on 'A is B' fail to learn 'B is A'},
    author={Berglund, L. and Holtzman, A. and Choi, Y. and Lee, S.},
    journal={arXiv preprint arXiv:2309.12288},
    year={2023}
}

@article{schaeffer2023emergent,
    title={Are Emergent Abilities of Large Language Models a Mirage?},
    author={Schaeffer, Rylan and Gokaslan, Braden and Brubaker, Ashley and Wen, Tiffany and Wong, Tong and Zhang, Caleb and others},
    journal={arXiv preprint arXiv:2304.15004},
    year={2023}
}

@article{ravi2025security,
    title={Security Degradation in Iterative AI Code Generation},
    author={Ravi, S. and Kumar, P. and Gupta, R.},
    journal={arXiv preprint arXiv:2506.11022},
    year={2025}
}

@article{espejel2025code,
    title={Code Generation with Small Language Models: A Deep Evaluation},
    author={Espejel, J. and Garcia, M. and Rodriguez, L.},
    journal={arXiv preprint arXiv:2504.07343},
    year={2025}
}

@article{zhu2024systematic,
    title={A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications},
    author={Zhu, Y. and Wang, Y. and Zhang, H. and Chen, J.},
    journal={arXiv preprint arXiv:2402.07927},
    year={2024}
}

@inproceedings{yin2024da,
    title={DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models},
    author={Yin, P. and Chen, Z. and Lin, J. and Han, W. and Feng, S.},
    booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
    year={2024}
}

@article{yin2025llms,
    title={Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation},
    author={Yin, P. and Li, Q. and Wang, H.},
    journal={arXiv preprint arXiv:2511.04355},
    year={2025}
}

@article{wang2024comprehensive,
    title={A Comprehensive Survey of Small Language Models in the Era of Large Language Models},
    author={Wang, Y. and Ma, L. and Li, X. and Zhang, C.},
    journal={arXiv preprint arXiv:2411.03350},
    year={2024}
}

@article{liu2025evaluation,
    title={Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices},
    author={Liu, H. and Zhang, M. and Chen, Z.},
    journal={arXiv preprint arXiv:2511.22138},
    year={2025}
}

@article{zhang2025revealing,
    title={Revealing the Power of Post-Training for Small Language Models in Edge Deployment},
    author={Zhang, X. and Wang, L. and Li, T.},
    journal={arXiv preprint arXiv:2509.26497},
    year={2025}
}

@article{kim2025slm,
    title={SLM-Bench: A Comprehensive Benchmark of Small Language Models for Efficiency and Impact},
    author={Kim, S. and Lee, J. and Park, H.},
    journal={arXiv preprint arXiv:2508.15478},
    year={2025}
}

@article{patel2025bridging,
    title={Bridging the Digital Divide: Small Language Models as a Pathway to Accessible AI},
    author={Patel, R. and Singh, A. and Sharma, S.},
    journal={arXiv preprint arXiv:2506.12403},
    year={2025}
}

@article{smith2025small,
    title={Small Language Models: Survey, Measurements, and Insights},
    author={Smith, J. and Brown, A. and Davis, C.},
    journal={arXiv preprint arXiv:2409.15790},
    year={2025}
}

@article{johnson2025end,
    title={An End-to-End Approach to Fine-Tune Small LLMs for Code Generation},
    author={Johnson, M. and Williams, L. and Miller, K.},
    journal={TechRxiv},
    year={2025}
}

@article{brown2025llms,
    title={LLMs and IoT: A Comprehensive Survey on Large Language Models Integration},
    author={Brown, T. and Jones, P. and Garcia, M.},
    journal={TechRxiv},
    year={2025}
}