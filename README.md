# A Semantic Round-trip Benchmark and Dataset

This repository contains the primary contributions of our research paper, "A Semantic Round-trip Benchmark and Dataset for Probing the Generalization Gap in Language Models."

The core contributions are:
1.  **A novel benchmark methodology**, the "Semantic Round-trip," designed to measure a model's **"Iterative Stability"**—a critical property for reliable, multi-step AI reasoning.
2.  **A public dataset** of over 7,000 experimental trial logs from 24 small language models (SLMs), providing detailed, cycle-by-cycle insights into model behavior under iterative stress.

Our analysis of the dataset reveals a significant performance gap between tasks with high prevalence in training data (e.g., `fizzbuzz`) and novel tasks of equivalent complexity. This finding highlights the utility of our benchmark in uncovering model behaviors that single-pass evaluations may miss and suggests that high performance on common problems may not always indicate true generalization. We release these resources to the community to facilitate further research.

## Overview

This project introduces a novel benchmark for evaluating the deterministic reliability of Small Language Models (SLMs) for code generation tasks. The primary goal is to assess how consistently a model can preserve the semantic meaning of a piece of code through a series of transformations, specifically in a resource-constrained context like edge AI.

Instead of testing a model's ability to solve a problem in a single pass, this benchmark focuses on **iterative stability**. It tests whether a model's understanding and generation capabilities remain stable over multiple cycles of transformation.

## Methodology

The core of this benchmark is the **Semantic Round-trip**. This process consists of a multi-turn cycle:

1.  **Code → Specification:** The model is given a Python function and asked to convert it into an abstract, language-agnostic specification (e.g., pseudocode, s-expression).
2.  **Specification → Code:** The model is then given the specification it just generated and asked to convert it back into functionally equivalent Python code.
3.  **Iteration & Validation:** The newly generated code becomes the input for the next cycle. This loop repeats up to 10 times. At each step, the output is validated against a ground truth (for specs) or by execution (for code). A failure at any point breaks the cycle.

This methodology is designed to detect subtle instabilities and deviations in a model's reasoning and generation capabilities that may not be apparent in single-pass evaluations.

A detailed flowchart of the process is available in the accompanying paper (`academic_paper_draft.md`).

### Evaluation Modes

We employ two primary evaluation modes to differentiate between a model's core logical capability and its output formatting stability:

-   **Strict Test:** Uses a basic parser that extracts the first Markdown code block from the model's response. This measures the model's raw reliability.
-   **Syntactic Forgiving Test:** Uses a more robust parser that leverages Python's `ast` module to find the first *syntactically valid* code block in the response. This test is designed to forgive "chattiness" or formatting errors and measure the underlying code generation capability.

## Key Findings from the Dataset

Our analysis of the dataset generated by the Semantic Round-trip benchmark highlights the utility of this methodology. The key finding is a **significant performance gap between known and novel tasks**, which suggests that many models rely on memorization rather than robust, generalizable reasoning.

-   **High Performance on Known Tasks:** High-performing models like `gemma3:4b` and `falcon3:3b` show high iterative stability on the `fizzbuzz` task, a problem likely common in training data. Success rates for these models can exceed 60-80%.
-   **Performance Collapse on Novel Tasks:** On a logically equivalent but novel task (`separate_vowels_and_consonants`), the performance of these same models collapses dramatically. For instance, `falcon3:3b`'s success rate drops from 61.7% to 0%.
-   **Instability as the Bottleneck:** This performance gap, revealed through our iterative methodology, suggests that the primary bottleneck for reliability is a fundamental lack of **"Iterative Stability"** on unfamiliar problems, rather than superficial formatting errors.

This core finding demonstrates the value of the Semantic Round-trip benchmark in probing for a deeper level of generalization than what is measured by single-pass evaluations. The full analysis and complete results are available in our accompanying paper.

## How to Run the Benchmark

1.  **Setup Environment:**
    *   Ensure you have a Python environment with the required packages installed.
        ```bash
        pip install -r requirements.txt
        ```
    *   **Set up Ollama:** This benchmark requires an Ollama-compatible API server.
        1.  **Install Ollama:** Download and install Ollama from the official website: [https://ollama.com/](https://ollama.com/)
        2.  **Pull Models:** Before running experiments, pull the models you wish to test. For example:
            ```bash
            ollama pull gemma3:4b
            ```
        3.  **Run Server:** The Ollama application usually runs the server automatically in the background. Ensure it is running and accessible from where you execute the benchmark scripts.

    *   **Set Environment Variables:**
        ```bash
        export OLLAMA_API_URL="http://localhost:11434/api/generate"
        # Optional: if you use a virtual environment
        # export VENV_PATH="/path/to/your/venv"
        ```

2.  **Run the Benchmark:**
    *   The primary execution script is `03_Scripts/run_all_experiments.sh`, which runs the robust **Syntactic Forgiving Test**.
    *   To run the test for all 24 default models:
        ```bash
        bash 03_Scripts/run_all_experiments.sh
        ```
    *   To run for a specific subset of models:
        ```bash
        bash 03_Scripts/run_all_experiments.sh "gemma3:4b" "llama2:7b"
        ```
    *   Results will be saved in a new timestamped directory inside `04_RawData/`.

3.  **Analyze Results:**
    *   The analysis script `analyze_with_ci.py` is located in the `05_Reports/` directory.
    *   This script is designed to be run from the project's root directory. It automatically finds the raw data in `04_RawData/` and generates an `analysis_with_ci.csv` file in the same `05_Reports/` directory.
        ```bash
        python3 05_Reports/analyze_with_ci.py
        ```

### Running Comparative Experiments

To reproduce the key comparative analysis discussed in the paper (`fizzbuzz` vs. `separate_vowels_and_consonants`), a centralized runner script is provided in the `03_Scripts/` directory.

**1. Execute the Script:**

The script can be run with command-line arguments to specify the number of runs and the models to test.

```bash
# Run the default experiment (30 runs for 3 key models)
./03_Scripts/run_all_experiments.sh

# Run a quick verification test (1 run for a single model)
./03_Scripts/run_all_experiments.sh --num-runs 1 "gemma3:4b"
```

-   **`--num-runs <integer>`:** (Optional) Specifies the number of trials. Defaults to 30.
-   **`[model_name...]`:** (Optional) A space-separated list of models to test. If omitted, it will use the default list defined inside the script (`falcon3:3b`, `gemma3:4b`, `llama3.2:3b`).

**2. Monitor and Analyze:**

The script will start all benchmark runs in the background and create log files for each in the `07_Logs/` directory. It will `wait` for all processes to complete. Once finished, you can analyze the results:

```bash
python3 05_Reports/analyze_with_ci.py
```

## Project Structure

A detailed description of each file and directory can be found in `FILE_DESCRIPTIONS.md`.

For a comprehensive explanation of the experimental scripts, data structure, and a full summary of results, please see the detailed documentation in [`05_Reports/experiment_documentation.md`](05_Reports/experiment_documentation.md).

### Experiment Scripts and Prompt Dependencies

This project contains several scripts to run different sets of experiments. The prompts used depend on the script being executed.

| Script | Target Test Case(s) | Spec Languages Used | Prompt Styles Used |
| :--- | :--- | :--- | :--- |
| `run_adaptive_benchmark.sh` | `process_user_list`, `separate_vowels_and_consonants` | `pseudocode` | `hyper_guided` |
| `run_benchmark.sh` | `get_magic_number` | `pseudocode`, `s_expression`, `minilang` | `fewshot`, `hyper_guided` |
| `run_fizzbuzz_benchmark.sh` | `fizzbuzz` | `pseudocode`, `s_expression`, `minilang` | `fewshot`, `hyper_guided` |

**Conclusion:** To ensure full reproducibility of all experiments defined in the `03_Scripts` directory, all existing prompt files under `02_Prompts/` are required and should not be deleted.
### Experiment Data and Script Compatibility\n\nTo clarify the relationship between the benchmark execution scripts, the raw data they generate, and the analysis script, the following table summarizes the current state of the repository. All scripts have been verified and are now fully consistent with the analysis pipeline.\n\n**Note on Historical Data:** Some directories in `04_RawData` (e.g., `strict_n_30_...`) were generated by previous versions of the benchmark scripts and potentially renamed. The main analysis script (`analyze_with_ci.py`) has been updated to correctly parse both these historical formats and the formats generated by the current scripts.\n\n| Execution Script (`03_Scripts/... `) | Generates Directory Pattern in `04_RawData/` | Parsed by `analyze_with_ci.py`? | Status |\n| :--- | :--- | :--- | :--- |\n| `run_fizzbuzz_benchmark.sh` | `run_fizzbuzz_{mode}_{timestamp}` | Yes | **Consistent** |\n| `run_adaptive_benchmark.sh` | `adaptive_{mode}_n{runs}_{test_case}_{timestamp}` | Yes | **Consistent** |\n| `run_all_experiments.sh` | (Calls `run_adaptive_benchmark.sh`) | Yes | **Consistent** |\n| `run_llama3_8b_tests.sh` | `llm_test_llama3-8b_{timestamp}` | Yes | **Consistent** |\n| `run_benchmark.sh` | `run_high_rep_{mode}_n{runs}_{timestamp}` | Yes | **Consistent** |\n| *(Historical Data)* | `strict_n_{runs}_{timestamp}` | Yes | Parsed correctly |\n| *(Historical Data)* | `forgiving_n_{runs}_{timestamp}` | Yes | Parsed correctly |\n\nAll execution scripts in the `03_Scripts` directory are now compatible with the `05_Reports/analyze_with_ci.py` analysis script.

## Dataset Availability

The comprehensive dataset generated by this benchmark, containing over 7,000 experimental trial logs from 24 small language models, is publicly available and citable via Zenodo:

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.18181174.svg)](https://doi.org/10.5281/zenodo.18181174)
